\documentclass[a4paper,UKenglish]{oasics}
%This is a template for producing OASIcs articles.
%See oasics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"

\usepackage{microtype}%if unwanted, comment out or use option "draft"

\usepackage{booktabs}

\usepackage{array}
\usepackage{tikz}
\usepackage{pgfplots}

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plain}% the recommended bibstyle


\newcommand{\todo}[1]{{\emph{TODO: #1}}}
\newcommand{\martin}[1]{{\color{blue} Martin: #1}}
\newcommand{\abcdef}[1]{{\color{red} Author2: #1}}

\newcommand{\code}[1]{{\small{\texttt{#1}}}}

% fatter TT font
\renewcommand*\ttdefault{txtt}

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{TACLeBench: A Benchmark Collection to Support  Worst-Case Execution
Time Research\footnote{This work was partially supported by COST Action
IC1202 Timing Analysis on Code-Level (TACLe).}}
%\titlerunning{TACLeBench} %optional, in case that the title is too long; the running title should fit into the top page column

% Heiko first, rest in alphabetical order
\author[1]{Heiko Falk}
\author[2]{Sebastian Altmeyer}
\author[3]{Peter Hellinckx}
\author[4]{Bj{\"o}rn Lisper}
\author[5]{Wolfgang Puffitsch}
\author[6]{Christine Rochange}
\author[5]{Martin Schoeberl}
\author[5]{Rasmus Bo S{\o}rensen}
\author[7]{Peter W{\"a}gemann}
\affil[1]{Hamburg University of Technology, Institute of Embedded Systems, Germany \\
  \texttt{Heiko.Falk@tuhh.de}}
\affil[2]{University of Amsterdam, The Netherlands\\
  \texttt{altmeyer@uva.nl}}
\affil[3]{University of Antwerp, iMinds, Belgium\\
  \texttt{peter.hellinckx@uantwerpen.be}}
\affil[4]{M{\"a}lardalen University, School of Innovation, Design, and Engineering, Sweden\\
  \texttt{bjorn.lisper@mdh.se}}
\affil[5]{Technical University of Denmark, Department of Applied Mathematics and Computer Science, Denmark\\
  \texttt{\{wopu, masca,rboso\}@dtu.dk}}
\affil[6]{University of Toulouse, France\\
  \texttt{rochange@irit.fr}}
\affil[7]{Friedrich-Alexander University Erlangen-NÃ¼rnberg, Germany\\
  \texttt{waegemann@cs.fau.de}}
  \authorrunning{H.~Falk et al.}%mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et. al.'

\Copyright{Heiko Falk et al.}%mandatory. OASIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\subjclass{Dummy classification -- please refer to \url{http://www.acm.org/about/class/ccs98-html}}% mandatory: Please choose ACM 1998 classifications from http://www.acm.org/about/class/ccs98-html . E.g., cite as "F.1.1 Models of Computation".
\keywords{Dummy keywords -- please provide 1--5 keywords}% mandatory: Please provide 1-5 keywords
% Author macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\serieslogo{}%please provide filename (without suffix)
\volumeinfo%(easychair interface)
  {Martin Schoeberl}% editors
  {2}% number of editors: 1, 2, ....
  {Conference/workshop/symposium title on which this volume is based on}% event
  {1}% volume
  {1}% issue
  {1}% starting page number
\EventShortName{}
\DOI{10.4230/OASIcs.xxx.yyy.p}% to be completed by the volume editor
% Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
Engineering related research, such as research on worst-case execution time,
uses experimentation to evaluate ideas. For these experiments we need
example programs. Furthermore, to make the research experimentation
repeatable those programs shall be made available.

We collected open-source programs, adapted them to a common coding
style, and provide the collection in open-source. The benchmark collection
is called TACLeBench and is available from GitHub in version 2.0
at the publication date of this paper.
\end{abstract}

\section{Notes}

\todo{Just internal notes to collect bullet points on ideas.}

\begin{itemize}
\item On issue with original benchmarks optimized away - we can now check with Patmos easily
\item Working on getting the license correct - issue with code from unknown source
\item Useful in general for embedded systems/barebone systems where less libraries are available
\item Usage of the benchmarks (version, no subsetting, no source change)
\item How to contribute
\end{itemize}

\section{Introduction}
\label{sec:intro}

\todo{Martin will work on this.}

Good, realistic benchmark suites are essential for the evaluation and comparison
of worst-case execution time (WCET) analysis, compiler, and computer architecture techniques.
TACLeBench provides a freely available and comprehensive benchmark suite
for timing analysis and related research topic.
%, featuring complex multi-core benchmarks in the near future.
TACLeBench will be continuously extended by novel benchmarks,
especially by parallel multi-task/multi-core benchmarks.
The extension of TACLeBench will be carefully managed with snapshots
and versioning so that it is clear which code has been used in a research
experiment.
The overall goal is to establish TACLeBench as the standard benchmarking
suite for WCET analysis, WCET oriented compiler and computer
architecture research worldwide.

TACLeBench is a collection of currently 56 benchmark programs
from several different research groups and tool vendors around the world.
These benchmarks are provided as ANSI-C 99 source codes.
The source codes are 100\% self-contained; no dependencies to system-specific
header files via \code{\#include} directives or an operating system exist.
All input data is part of the C source code.
Eventually used functions from math libraries are also provided in the form of C source code. Furthermore, almost all benchmarks are processor-independent and can
be compiled and evaluated for any kind of target processor.
The only exception is PapaBench which uses architecture-dependent
I/O addresses and which currently supports Atmel AVR processors.

Since TACLeBench addresses the needs imposed by timing analysis tools,
all benchmarks are completely annotated with flow facts.
These flow facts are directly incorporated into the ANSI-C source codes using
Pragmas at a high level of abstraction.
For example, the following source code snippet states that the shown loop
iterates between 50 and 100 times, depending on the data dependency
via variable \code{maxIter}:

\begin{verbatim}
      _Pragma( "loopbound min 50 max 100" )

      for ( i = 1; i <= maxIter; i++ )

        Array[i] = i * fact * KNOWN_VALUE;

\end{verbatim}


The complete description of the used flow fact language is part of the source
distribution. If you would like to share your very own benchmarks with us,
feel free to contact Heiko Falk (or any coauthor of this paper) in order
to have your source codes included in TACLeBench.


This paper... \todo{purpose statement, latest in 4th paragraph}

\todo{We are aiming on self-contained benchmarks, that do not need any
operating system services, including file IO. Therefore,...}

\todo{Talk about the TACLe COST action and the subgroup around the benchmark
collection. And plans how to further develop and maintain the collection.}

\todo{Talk about usage of the benchmarks: WCET tools, compiler, hardware
architecture,...}

The first version of TACLeBench (version 1.0, available
from\footnote{\url{http://www.tacle.eu/index.php/activities/taclebench}}), which was produced
by Heiko Falk, was a collection of 102 benchmark programs from several different
research groups. We keep this first version tagged with ``V1.0'' in the public
GitHub repository.\footnote{\url{https://github.com/tacle/tacle-bench}}
The version described in this paper is version 2.0 and tagged as such in the
repository.
The intention is to have the HEAD of the master branch being
the most recent, versioned snapshot of TACLeBench.
Future development and additions will be performed on a development
branch so that HEAD is always a consistent snapshot.

The contributions of this paper are: (1) ... (2) ...

This paper is organized in N sections: The following section presents related work.
Section~\ref{sec:related} presents related work.
Section~\ref{sec:collect} presents the benchmark collection, its classification, and the updates
to make them useful.
Section~\ref{sec:eval} evaluates the benchmark collection on...
Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related}

The M{\"a}lardalen WCET benchmarks (MRTC)~\cite{wcet:bench:2012} is the first collection of programs especially
intended for benchmarking WCET analysis tools, with a focus on program flow analysis.
This collection of C programs was an attempt to organize a number of codes, which had up to then been floating around
in the research community, into a repository with a more systematic treatment of meta-data and easy web access.
It was collected from several sources in 2005, and has since then been
used in many WCET research projects as well as for the WCET Tool Challenge 2006~\cite{Gustafsson:ISOLA2006}.
A subset of the M{\"a}lardalen benchmarks has even been translated to
Java~\cite{jop:volta:rtas2008}. Most benchmarks are relatively small, except
two C programs that have been generated from tools. The benchmarks also
contain all input data. This effectively turns them into single-path programs, which
makes them less suitable for evaluating tools that can handle multi-path codes. To remedy that,
some of the benchmarks are equipped with suggested input ranges that override the fixed
input data in the code. The user is however then responsible to use the proper settings of the tool,
or to create an appropriate test harness, such that the programs are analyzed with the specified input ranges
rather than the fixed input data. Another consequence of the fact that the
input data is fixed, and that some programs do not provide
any return value, is that good compilers with optimization turned on can optimize
most of the code away. We include most of the benchmarks from the
 M{\"a}lardalen WCET benchmark suite in TACLeBench.
 However, to prohibit the unwanted compiler optimizations we changed
 the way input data is represented in variables (made them \code{volatile}),
 and made the return of \code{main} dependent on the benchmark calculation.
 Furthermore, we dropped benchmarks where the licensing terms are unknown
 or even disallow distributing the source.


MiBench~\cite{MiBench} is a collection of benchmarks targeting the embedded
domain and providing them in open-source.
We include some of the MiBench benchmarks, especially
those where it was possible to include the input data with the C source.

DEBIE~\cite{debie} is a software derived from a satellite-mounted detector of micro-meteorids and space debris.
It was developed by Space Systems Finland Ltd. and was converted by Tidorum Ltd. into a portable benchmark for real-time applications.
DEBIE is a multi-task application that consists of only 8, and partially very small, different tasks.
DEBIE is accompanied by a specification of valid input and ouput data and of required activation rates of the individual tasks.

PapaBench~\cite{papabench} has been derived from  Paparazzi\footnote{\url{https://wiki.paparazziuav.org/}},
an open-source hardware and software project for the design of unmaned aerial vehicles.  PapaBench includes
two software components of this project, that run on separate processors communicating through an SPI link:
the \textit{fly-by-wire} part controls the flight (engine and flaps control, stabilization, radio-communication
with the ground, support for infrared sensors, etc.) while the \textit{autopilot} part controls the GPS and
executes the flight plan (which is decided offline). Both software parts  cumulate 13 tasks that are subject
to precedence constraints and 6 interrupt service routines.

The Embedded Microprocessor Benchmark Consortium (EEMBC)~\cite{eembc} provides their own benchmark suite dedicated to the evaluation of
the performance of embedded hardware and embedded software.
The benchmarks are divided into subset according to the target domain, e.g., the automotive domain, phones and tables, but also big data and cloud computing.
To improve comparability between different processors and systems, the consortium provides a sophisticated test-harness which allows to derive certifiable scores.
The test-harness, being a clear advantage in terms of comparability and certifiability, constitutes a hindrance in terms of portability and usability.
Whereas the TACLeBench has been designed to ease portability and to allow the immediate use of the benchmark with a large variety of tools and platforms, the EEMBC benchmarks are not stand-alone executable without the test-harness.
Furthermore, in stark contrast to the aforementioned benchmark suites, and especially to TACLeBench, the EEMBC benchmarks are not published under an open-source license.
Instead, the benchmarks are behind a pay-wall, even for purely academic research.


JemBench~\cite{jembench} is a Java benchmark suite targeting
embedded Java platforms. JemBench only assumes the
availability of a CLDC API, the minimal configuration
defined for the J2ME. The core of the benchmark suite consists of
adapted real-world applications.
The benchmarks are structured in {\em micro}, {\em kernel}, {\em
application}, {\em parallel}, and {\em streaming} benchmarks.
Micro benchmarks are used to measure short bytecode sequences;
kernel benchmarks compute a computational kernel; and application
benchmarks are real-world programs restructured as standalone benchmarks.
Parallel and streaming benchmarks are intended to explore multicore
speedup.
\martin{Maybe we should port some of those to C?}
Benedikt Huber ported one of the application benchmarks (Lift) to C
and we include it in TACLeBench.




\section{The Benchmark Collection}
\label{sec:collect}


\subsection{Benchmark Sources}

\todo{what did we collect. How many indirections in collections have there be?
E.g., adpcm.c: (1) SOURCE : C Algorithms for Real-Time DSP by P. M. Embree,
(2) SNU-RT Benchmark Suite, (3) MDH, and (4) TACLeBench. How have
benchmarks  changed? Legal changes?}

\subsection{Classification}

\todo{Talk about kernel, medium size, and application benchmarks and their usage.}
\todo{The \# of lines of code was found using the linux utility \code{slocclount} and is measured in source lines of code (SLoC).}
\begin{itemize}
\item Static analysis
\item Measurement based analysis
\item Hybrid analysis: The hybrid model splits the code of a set of tasks into so-
called basic blocks. Each basic block contains one path trace of instructions with a single set of inputs and a single set of outputs.  The  size  of  a  basic  block  varies  between  the  largest possible trace meeting the constraints of a single set of inputs and outputs over a single memory block, i.e. a unit of memory that is replaced by a cache replacement algorithm, to a single program instruction.
The  challenge  of  this  two  layer  hybrid  approach  is  tackling the computational complexity problems within the static analysis  layer  and  the  accuracy  within  the  measurements based  layer.  In  other  words,  a  proper  balance  needs  to  be found between the two layers of the hybrid model. The TACLeBench is used in the COBRA-HPA (COde Behaviour fRAmework-Hybrid Program Analyser) framework which facilitates evaluation of the different approaches using different block sizes.
\item Schedulability analysis using TACLeBench is facilitated by the the COBRA-TG (Taskset Generator). Different scheduling methodologies can be analysed in a reproducible way using generated tasksets based on specific application descriptions. It replaces the limited set of universal scheduling benchmarks and the wide variety of non-reproducible benchmark application with a methodology that enables for an unlimited but reproducible set of tasksets.

Based on taskset descriptions
\end{itemize}

\todo{The following is a verbatim copy from the TACLe website.
Needs to be updated and put into a nice table, like the following template.}

\begin{table}
\centering
\caption{\label{tab:bench_kernel}TACLeBench kernel benchmarks}  
\begin{tabular}{lp{6cm}rr}
\toprule
Name & Description & Code Size & Origin\\
     &             &     (SLoC) & \\ \midrule
binarysearch & Binary search of 15 integers & 67 & MRTC \\
bitcount & Couting number of bits in an integer array & 230  & snippest \\
bitonic &  Bitonic sorting network  & 73 & MiBench \\
bsort & Bubblesort program & 60 & MRTC \\
complex\_updates &  Multiply-add with vectors of complex numbers & 59 & DSPStone \\
countnegative &  Counts non-negative numbers in a matrix  & 78 & MRTC \\
crc &  CRC computation on 40 bytes of data  & 81 & SNU-RT \\
fac &  Factorial function & 40 & MRTC \\
fft &  1024-point FFT, 13 bits per twiddle  & 488  & DSPStone \\
filterbank & Filter bank for multirate signal processing  & 89 & StreamIt \\
fir2dim &  2-dimensional FIR filter convolution & 119 & DSPStone \\
iir &  Biquad IIR 4 sections filter & 67 & DSPStone \\
insertsort & Insertion sort & 70 & MRTC \\
jfdctint & Discrete-cosine transformation on a 8x8 pixel block  & 150  & MRTC \\
lms &  LMS adaptive signal enhancement  & 160  & MRTC \\
ludcmp & LU decomposition & 111 & MRTC \\
matrix1 &  Generic matrix multiplication  & 63 & DSPStone \\
md5 &  Message digest algorithm & 355  & NetBench \\
minver & Floating point matrix inversion  & 176  & MRTC \\
pm & Pattern match kernel & 1571  & HPEC \\
prime &  Prime number test  & 67 & MRTC \\
quicksort &  Quick sort of string arrays  & 1613 & MiBench \\
recursion &  Artificial recursive code  & 36 & MRTC \\
select & Select the Nth largest number in a floating point array  & 102 & MRTC \\
sha &  NIST secure hash algorithm & 2049  & MiBench \\
st & Statistics program (sum, mean, variance, std. deviation, correlation)  & 127  & MRTC \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{\label{tab:bench_sequential}TACLeBench sequential benchmarks}  
\begin{tabular}{lp{5cm}rr}
\toprule
Name & Description & Code Size & Origin\\
   & &  (SLoC) & \\ \midrule
adpcm\_dec & ADPCM decoder & 398 & SNU-RT \\
adpcm\_enc &  ADPCM encoder  & 410  & SNU-RT \\
ammunition & C compiler arithmetic stress test  & 2448 & DINO \\
anagram &  Word anagram computation & 2772  & Raymond Chen \\
audiobeam &  Audio beam former  & 6653  & StreamIt \\
cjpeg\_transupp &  JPEG image transcoding routines  & 673 & MediaBench \\
cjpeg\_wrbmp & JPEG image bitmap writing code & 965 & IJG \\
dijkstra & All pairs shortest path  & 247  & MiBench \\
epic & Efficient pyramid image coder  & 979  & MediaBench \\
fmref &  Software FM radio with equalizer & 680  & StreamIt \\
g723\_enc &  CCITT G.723 encoder  & 546  & SUN Microsystems \\
gsm\_dec & GSM provisional standard decoder & 1426 & MediaBench \\
gsm\_enc & GSM provisional standard encoder & 1951 & MediaBench \\
h264\_dec &  H.264 block decoding functions & 1053 & MediaBench \\
huff\_dec &  Huffman decoding with a file source to decompress  & 242  & David Bourgin \\
huff\_enc & Huffman encoding with a file source to compress  & 386  & David Bourgin \\
mpeg2 &  MPEG2 motion estimation  & 12601 & MediaBench \\
ndes & Complex embedded code  & 299  & MRTC \\
petrinet & Petri net simulation & 500  & MRTC \\
rijndael\_dec & Rijndael AES decryption  & 2922 & MiBench \\
rijndael\_enc & Rijndael AES encryption  & 2749 & MiBench \\
statemate &  Statechart simulation of a car window lift control & 1107 & MRTC \\
susan &  MR image recognition algorithm & 8782 & MiBench \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\centering
\caption{\label{tab:bench_parallel}TACLeBench parallel benchmarks}  
\begin{tabular}{lp{5cm}rrr}
\toprule
Name & Description & \#Tasks &Code Size & Origin\\
     &             &         &(SLoC) & \\ \midrule
Debie&  DEBIE-1 instrument observing micro-meteoroids and small space debris &8 &  6615 & Tidorum Ltd \\
PapaBench &  UAV autopilot and fly-by-wire software & 10 & 6336 & Paparazzi \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{\label{tab:bench_app}TACLeBench application benchmarks}  
\begin{tabular}{lp{5cm}rrr}
\toprule
Name & Description & \#Tasks & Code Size & Origin\\

     &             &         &     (SLoC) & \\ \midrule
lift & A lift controller & 1 & 395 & Martin Schoeberl\\
Powerwindow & Distributed powerwindow control software & 5 & 1869 & CoSys-Lab \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{\label{tab:bench_test}TACLeBench test benchmarks}  
\begin{tabular}{lp{5cm}rp{2.5cm}}
\toprule
Name & Description & Code Size & Origin\\
     &             &  (SLoC) & \\ \midrule
cover &  Artificial code with lots of different control flow paths  & 636  & MRTC \\
duff & Duff's device  & 67 & MRTC \\
test3 &  Artificial WCET analysis stress test & 4297 & Universitaet des Saarlandes \\
\bottomrule
\end{tabular}
\end{table}




\subsection{Issues with the Original Sources}


\todo{Show the issues with code snippets}

\begin{itemize}
\item Compiler optimization
\item byte order
\item low-level access to device registers
\item Some benchmarks, which are available for download in the Internet have a license that
does not allow open-source distribution. We removed those benchmarks from the original source.
\end{itemize}

\subsection{Benchmark Changes}



\begin{itemize}
\item unique names for functions and global variables
\item Sometimes movements of stack allocated variables into global
\item splitting of initialization and computation code into two functions
\item Loop bounds added
\end{itemize}


\subsection{Usage Recommendations}

\todo{Mmh shall we have this?}

\todo{can we include a paragraph about the taskset generator for schedulability analysis}

\begin{itemize}
\item Use a stable version. At the time of this writing we should have V 2.0
\item Do not edit the benchmarks
\item Use all benchmarks in your evaluation. Don't introduce a bias in your evaluation by
\emph{selecting the most representative benchmarks} for your improvement
\end{itemize}

\subsection{Licenses}

An issue we encountered with several benchmarks was that the original
source did not include any licensing information. In absence of such
information, we had to assume that the copyright holder reserves all
rights. Wherever necessary, we contacted the copyright holders to
obtain the right to use, modify, and redistribute the benchmarks. In a
small number of cases we however discovered that the code was in fact
under a license that made the benchmark unusable; the respective
benchmarks were consequently dropped from the TACLeBench benchmark
suite. All benchmarks in the benchmark suite now contain licensing
information, such that future developments do not require tracking
down the original authors of the benchmark.

\section{Evaluation}
\label{sec:eval}


\todo{Properly discuss Figure~\ref{fig:execution-times}}
\begin{figure}[t]
  \def\resultfile{eval/wcet.csv}
  \input{eval/pasim-eval.tikz}
  \caption{Execution times in TACLeBench vary from \todo{x} to \todo{y} on the Patmos architecture.}
  \label{fig:execution-times}
\end{figure}

\subsection{Sanity Checks}
All code-changes were done manually by different members of the TACLe Cost action.
Such a collaborative procedure is inherently error-prone.
For once, human errors can occur and the original sources were often faulty to start with.
The distributive work on the benchmarks led to an additional source of errors:
Different system configurations can lead to different behaviors, and a benchmark behaving well under system configuration A may be faulty under configuration B.
To ensure the quality of the benchmarks and to improve portability, we have thus implemented automatic sanity checks.

The code quality and conformity with the code formatting rules is validated centrally and the latest results can be viewed online\footnote{\url{http://tacle.cs.fau.de/}}.
For this sanity check, all benchmarks are compiled using \textit{gcc}, \textit{g$++$} and \textit{clang}, executed and the return values are checked against the expected value.
Clang's static analyzer\footnote{\url{http://clang-analyzer.llvm.org/}} is used to further reveal programming bugs, such as out-of-bounds errors, and also to validate the compliance of the code with the code-formatting rules.

The portability has been checked via a shell-script \textit{checkbenchmarks.sh} that is now part of the TACLeBench git-repository.
The script allows to quickly identify incompatibilities of the benchmarks with specific operating systems, compilers and system configurations.
Even though full coverage of all system configurations can never be achieved, we were able to cover most of the common operating systems and compilers.

% In order to improve the code quality of the TACLeBench suite, we used different
% compilers (e.g., clang, gcc) to identify shortcomings in the code of each benchmark in an
% automated way. Additionally, we exploited Clang's static
% analyzer\footnote{\url{http://clang-analyzer.llvm.org/}} to further reveal
% programming bugs in the benchmark suite. We extended this sanity checker with additional
% TACLeBench-specific passes to identify if all benchmark are compliant to the code-formatting rules.

\todo{Comparison compiler warning in TACLeBench v1 with v2, problem: expint was in 1.0, but is no
longer in 2.0 and PowerWindow was added for 2.0}

Possible Options for the Evaluation:

\begin{itemize}
\item Compile for an embedded processor and measure execution time
\item Compile for Patmos and use pasim for execution cycles
\item Show how compiler optimizations optimized away the original code and how we fixed this
\item Complexity numbers with bar charts on execution time and maybe tools
\item Use some WCET tools
\end{itemize}


\section{Conclusion}
\label{sec:conclusion}

\todo{Rephrase what this paper is about and list the main contributions and results.}

\subparagraph*{Acknowledgements}

\todo{TACLe COST action phrase}

We want to thank Bendikt Huber for porting the Lift benchmark from Java to C.
We want to thank Niklas Holsti from Tidorum Ltd for contributing DEBIE in open-source.
% Order according to contribution count on GitHub: https://github.com/tacle/tacle-bench/graphs/contributors
% But Simon is not listed there! Maybe we are missing some people?
We want to thank Yorick De Bock, J{\"o}rg Mische, Simon Wegener, and Haoxuan Li for helping in restructuring the benchmarks.

\appendix
\section{Source Access and Compiling the Benchmarks}


\bibliography{taclebench}


\end{document}
